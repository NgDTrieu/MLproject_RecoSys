{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom libraries\n",
    "sys.path.append('../Util')\n",
    "from loader import get_books, get_book_dataframe, get_book_features\n",
    "from joiner import get_ratings, get_joint, load_amazon, load_goodreads\n",
    "from reduction import reduce_matrix, get_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_user_to_features(p, features):\n",
    "    p_sparse = scipy.sparse.csr_matrix(p)\n",
    "    # map new user to concept space by p*features\n",
    "    user_to_concept = p_sparse.dot(features)\n",
    "    # map user back to itme space with user_to_concept * featuresT\n",
    "    result = user_to_concept.dot(features.T).todense()\n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(p, q, user_bias, item_bias, global_bias):\n",
    "    pred_ratings = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        pred = global_bias + user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "        # pred = global_bias + user_bias + np.dot(p, q[i])\n",
    "        pred_ratings[i] = pred\n",
    "    return pred_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recs(result, books, n, q):\n",
    "    recs = []\n",
    "    for i in range(len(result)):\n",
    "        if q[i] == 0: # book user hasn't already rated\n",
    "            recs.append((i, result[i]))\n",
    "        else:\n",
    "            recs.append((i, float('-inf'))) \n",
    "            # recs.append((i, result[i])) #leave this to verify things actually working\n",
    "    recs = sorted(recs, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    top_titles = []\n",
    "    for i in range(n):\n",
    "        book_id = recs[i][0]\n",
    "        title = books.iloc[book_id]['title']\n",
    "        top_titles.append(title)\n",
    "    return top_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to where you save and load all data\n",
    "data_path = '../../goodbooks-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found books_dataframe in file...\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe from books\n",
    "books = get_book_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cu2rec components\n",
    "filename = '../.tmp/goodbooks_sorted_f300'\n",
    "q = genfromtxt('{}_q.csv'.format(filename), delimiter=',')\n",
    "item_bias = genfromtxt('{}_item_bias.csv'.format(filename), delimiter=',')\n",
    "\n",
    "\n",
    "# surprise components\n",
    "# filename = '../.tmp/svd_100_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/Q_300.npy'\n",
    "# q = np.load(filename)\n",
    "# filename = '../.tmp/item_bias_300.npy'\n",
    "# item_bias = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert global bias to float - get from whatever dataset you used\n",
    "global_bias = 3.919866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user from goodreads\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_mystery_scifi_hates_fantasy.npz')\n",
    "# sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_likes_fantasy.npz')\n",
    "sparse_new_user_scaled = scipy.sparse.load_npz('../.tmp/cached_users/user_nickgreenquist.npz')\n",
    "new_user_ratings_scaled = sparse_new_user_scaled.toarray()\n",
    "new_user_ratings_scaled = np.array(new_user_ratings_scaled[0].tolist())\n",
    "new_user_ratings = np.copy(new_user_ratings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undo the rating mapping we usually do\n",
    "\n",
    "# Turn 1-5 rating scale into negative - positive scale\n",
    "# original mapper: ratings_mapper = {0:0, 1:-2, 2:-1, 3:1, 4:2, 5:3}\n",
    "ratings_mapper = {0:0, -2:-1, -1:-2, 1:3, 2:4, 3:5}\n",
    "for i in range(len(q)):\n",
    "    new_user_ratings[i] = ratings_mapper[new_user_ratings_scaled[i]]\n",
    "new_user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array of indices of books this user has actually rated\n",
    "indices = []\n",
    "for i in range(len(new_user_ratings)):\n",
    "    if new_user_ratings[i] != 0:\n",
    "        indices.append(i)\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "learning_rate = 0.07\n",
    "user_bias_reg = 0.002\n",
    "P_reg = 0.0002\n",
    "\n",
    "# updates per rating\n",
    "iterations = 25\n",
    "\n",
    " # how many iterations to see the total loss at this step - remove in webapp!\n",
    "calculate_total_loss = float('inf')\n",
    "\n",
    "n_factors = q.shape[1]\n",
    "cols = q.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set the user_bias for this user\n",
    "new_user_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. set up new random P\n",
    "mu, sigma = 0, 0.1\n",
    "p = np.random.normal(mu, (sigma / n_factors), n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE at Iteration 0: 1.7627478538334462\n",
      "RMSE at Iteration 24: 0.3261305490339041\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. computer small number of iterations of SGD\n",
    "for iteration in range(iterations):\n",
    "    \n",
    "    #= periodically calculate total loss and output\n",
    "    if iteration == 0 or iteration == iterations - 1 or iteration % calculate_total_loss == 0:\n",
    "        total_loss = 0.0\n",
    "        for i in indices:\n",
    "            rating = new_user_ratings[i]\n",
    "            pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "            # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "            error = rating - pred\n",
    "            total_loss += pow(error, 2)\n",
    "            \n",
    "        rmse = math.sqrt(total_loss / len(indices))\n",
    "        print(\"RMSE at Iteration {}: {}\".format(iteration, rmse))\n",
    "\n",
    "    # i = random.choice(indices) # SGD\n",
    "    for i in indices: # Gradient Descent using every book per iteration\n",
    "    \n",
    "        # calculate loss on random item\n",
    "        rating = new_user_ratings[i]\n",
    "        pred = global_bias + new_user_bias + item_bias[i] + np.dot(p, q[i])\n",
    "        # pred = global_bias + new_user_bias + np.dot(p, q[i])\n",
    "        error = rating - pred\n",
    "\n",
    "        # update P\n",
    "        for f in range(n_factors):\n",
    "            p_update = learning_rate * (error * q[i][f] - P_reg * p[f])\n",
    "            p[f] += p_update\n",
    "\n",
    "        # update user bias\n",
    "        ub_update = learning_rate * (error - user_bias_reg * new_user_bias)\n",
    "        new_user_bias += ub_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using partial fit\n",
    "predictions_partial_fit = get_predictions(p, q, new_user_bias, item_bias, global_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The School of Essential Ingredients\n",
      "The Count of Monte Cristo\n",
      "Gabriel's Inferno (Gabriel's Inferno, #1)\n",
      "You Can Heal Your Life\n",
      "Your Inner Fish: A Journey into the 3.5-Billion-Year History of the Human Body\n",
      "Someday, Someday, Maybe\n",
      "Ahab's Wife, or The Star-Gazer\n",
      "American Pastoral (The American Trilogy, #1)\n",
      "West with the Night\n",
      "These Is My Words: The Diary of Sarah Agnes Prine, 1881-1901, Arizona Territories (Sarah Agnes Prine, #1)\n",
      "Small Gods (Discworld, #13)\n",
      "The Black Stallion (The Black Stallion, #1)\n",
      "Thirteen Reasons Why\n",
      "Where the Mountain Meets the Moon\n",
      "The 4-Hour Workweek\n",
      "Stranger in a Strange Land\n",
      "Rubyfruit Jungle\n",
      "The Book of Mormon: Another Testament of Jesus Christ\n",
      "The End of Eternity\n",
      "Not Without My Daughter\n",
      "Definitely Dead (Sookie Stackhouse, #6)\n",
      "Crown Duel (Crown & Court #1-2)\n",
      "The Year of Living Biblically: One Man's Humble Quest to Follow the Bible as Literally as Possible\n",
      "Comfort Me with Apples: More Adventures at the Table\n",
      "My Booky Wook\n"
     ]
    }
   ],
   "source": [
    "# print out top results using just partial fit predictions\n",
    "recs_partial_fit = get_top_n_recs(predictions_partial_fit, books, 25, new_user_ratings)\n",
    "for rec in recs_partial_fit:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCombine recs from partial fit with recs from mapping to feature matrix using log_rank\\n\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Combine recs from partial fit with recs from mapping to feature matrix using log_rank\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_matrix exists in file...\n"
     ]
    }
   ],
   "source": [
    "# produce feature matrix\n",
    "feature_matrix = get_book_features(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions using feature matrix\n",
    "predictions_features = map_user_to_features(new_user_ratings, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLog Ranking\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Log Ranking\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tuple of book_id and rating for each method, then sort\n",
    "partial_fit_ratings = []\n",
    "feature_ratings = []\n",
    "for i in range(len(books)):\n",
    "    partial_fit_ratings.append((i, predictions_partial_fit[i]))\n",
    "    feature_ratings.append((i, predictions_features[i]))\n",
    "\n",
    "partial_fit_ratings = sorted(partial_fit_ratings, key=lambda x: x[1], reverse=True)\n",
    "feature_ratings = sorted(feature_ratings, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map book_id to the rank for each method\n",
    "id_to_rank_partial_fit = {}\n",
    "id_to_rank_features = {}\n",
    "for i in range(len(books)):\n",
    "    book_id = partial_fit_ratings[i][0]\n",
    "    id_to_rank_partial_fit[book_id] = math.log(i+1)\n",
    "\n",
    "    book_id = feature_ratings[i][0]\n",
    "    id_to_rank_features[book_id] = math.log(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9798\n"
     ]
    }
   ],
   "source": [
    "weight_feature = 0.5\n",
    "\n",
    "rankings = []\n",
    "for i in range(len(books)):\n",
    "    if new_user_ratings[i] == 0:\n",
    "        rank = weight_feature*id_to_rank_features[i] + (1.0-weight_feature)*id_to_rank_partial_fit[i]\n",
    "        rankings.append((rank, i))\n",
    "rankings = sorted(rankings, key=lambda x: x[0])\n",
    "print(len(rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_books = []\n",
    "for i in range(100):\n",
    "    book_id = rankings[i][1]\n",
    "    book = books.iloc[book_id]\n",
    "    book['rank'] = i + 1\n",
    "    top_books.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Count of Monte Cristo\n",
      "The Sword in the Stone (The Once and Future King, #1)\n",
      "Your Inner Fish: A Journey into the 3.5-Billion-Year History of the Human Body\n",
      "Grendel\n",
      "The Amulet of Samarkand (Bartimaeus, #1)\n",
      "The Neverending Story\n",
      "Dandelion Wine (Green Town, #1)\n",
      "The Lost Gate (Mither Mages, #1)\n",
      "The Earthsea Trilogy\n",
      "Someday, Someday, Maybe\n",
      "The End of Eternity\n",
      "Flowers for Algernon\n",
      "A Man in Full\n",
      "A Wrinkle in Time (A Wrinkle in Time Quintet, #1)\n",
      "The Alchemyst (The Secrets of the Immortal Nicholas Flamel, #1)\n",
      "Stranger in a Strange Land\n",
      "A Wizard of Earthsea (Earthsea Cycle, #1)\n",
      "The Magician's Land (The Magicians, #3)\n",
      "J.R.R. Tolkien 4-Book Boxed Set: The Hobbit and The Lord of the Rings\n",
      "Crown Duel (Crown & Court #1-2)\n",
      "The Queen of Attolia (The Queen's Thief, #2)\n",
      "An Acceptable Time (A Wrinkle in Time Quintet, #5)\n",
      "An Ember in the Ashes (An Ember in the Ashes, #1)\n",
      "Where the Mountain Meets the Moon\n",
      "Alcatraz Versus the Evil Librarians (Alcatraz, #1)\n",
      "The Blade Itself (The First Law, #1)\n",
      "West with the Night\n",
      "You Can Heal Your Life\n",
      "The Black Stallion (The Black Stallion, #1)\n",
      "Many Waters (A Wrinkle in Time Quintet, #4)\n",
      "The 4-Hour Workweek\n",
      "A Swiftly Tilting Planet (A Wrinkle in Time Quintet, #3)\n",
      "A Wind in the Door (A Wrinkle in Time Quintet, #2)\n",
      "Tuck Everlasting\n",
      "The Book of Three (The Chronicles of Prydain, #1)\n",
      "The Shadow of the Torturer (The Book of the New Sun #1)\n",
      "The Magician King (The Magicians, #2)\n",
      "The School of Essential Ingredients\n",
      "My Side of the Mountain (Mountain, #1)\n",
      "The Year of Living Biblically: One Man's Humble Quest to Follow the Bible as Literally as Possible\n",
      "Split Infinity  (Apprentice Adept #1)\n",
      "Thirteen Reasons Why\n",
      "The Curious Case of Benjamin Button\n",
      "Last Argument of Kings (The First Law, #3)\n",
      "Gabriel's Inferno (Gabriel's Inferno, #1)\n",
      "Un Lun Dun\n",
      "Memories of Ice (The Malazan Book of the Fallen, #3)\n",
      "A Spell for Chameleon (Xanth #1)\n",
      "Defiance (Defiance #1)\n",
      "The Bone Clocks\n",
      "Talking to Dragons (Enchanted Forest Chronicles, #4)\n",
      "The Power That Preserves (The Chronicles of Thomas Covenant the Unbeliever, #3)\n",
      "Lord Foul's Bane (The Chronicles of Thomas Covenant the Unbeliever, #1)\n",
      "Froi of the Exiles (Lumatere Chronicles, #2)\n",
      "Wondrous Strange (Wondrous Strange, #1)\n",
      "Cheaper by the Dozen\n",
      "The Chrysalids\n",
      "Ahab's Wife, or The Star-Gazer\n",
      "Just So Stories\n",
      "Howl's Moving Castle (Howl's Moving Castle, #1)\n",
      "All the Birds in the Sky\n",
      "Beautiful Darkness (Caster Chronicles, #2)\n",
      "Animal Farm / 1984\n",
      "Harry Potter Boxset (Harry Potter, #1-7)\n",
      "The Bees\n",
      "The Magician (The Secrets of the Immortal Nicholas Flamel, #2)\n",
      "Dry\n",
      "Same Kind of Different as Me\n",
      "The Lost World (Professor Challenger, #1)\n",
      "The Last Dragonslayer (The Chronicles of Kazam, #1)\n",
      "Brisingr (The Inheritance Cycle, #3)\n",
      "His Dark Materials (His Dark Materials #1-3)\n",
      "The Dragon Heir (The Heir Chronicles, #3)\n",
      "In a Dark, Dark Wood\n",
      "The Godfather\n",
      "Ozma of Oz (Oz, #3)\n",
      "The Hitchhiker's Guide to the Galaxy: A Trilogy in Four Parts\n",
      "American Pastoral (The American Trilogy, #1)\n",
      "The Robe\n",
      "The Illearth War (The Chronicles of Thomas Covenant the Unbeliever, #2)\n",
      "Clockwork Angel (The Infernal Devices, #1)\n",
      "Rubyfruit Jungle\n",
      "If You Ask Me (And of Course You Won't)\n",
      "The Hero and the Crown (Damar, #2)\n",
      "The High Lord (Black Magician Trilogy, #3)\n",
      "The Call of the Wild\n",
      "The Blue Sword (Damar, #1)\n",
      "Little Children\n",
      "Childhood's End\n",
      "The Girl Who Circumnavigated Fairyland in a Ship of Her Own Making (Fairyland, #1)\n",
      "Calamity (The Reckoners, #3)\n",
      "Wuthering Heights\n",
      "The Warrior Heir (The Heir Chronicles, #1)\n",
      "The Interestings\n",
      "The Riddle (The Books of Pellinor, #2)\n",
      "Death Comes for the Archbishop\n",
      "The Scar (Bas-Lag, #2)\n",
      "The Last Unicorn (The Last Unicorn, #1)\n",
      "The Invasion of the Tearling (The Queen of the Tearling, #2)\n",
      "Best Served Cold\n"
     ]
    }
   ],
   "source": [
    "for book in top_books:\n",
    "    print(book['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
